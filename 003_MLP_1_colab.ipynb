{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "003_MLP_1_colab",
      "provenance": [],
      "authorship_tag": "ABX9TyMtOjcbcUHWDk7utGDpPkqT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/perezm-496/AI4MedImg/blob/main/003_MLP_1_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPNgAbOZoS04"
      },
      "source": [
        "# Multilayer Perceptron\n",
        "\n",
        "- We create a two classes dataset with random data, we make sure to make it non linear separable.\n",
        "- We use a simple architecture of a MLP with three layers: 2 - 16 - 2 - 1.\n",
        "- We make use of ReLu as the activation function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhqgI7NTn_C2"
      },
      "source": [
        "\"\"\"Random data creation, two non linearly separable classes.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(5)\n",
        "\n",
        "# lets define a boundary given by x^2 - x^3 + 0.5\n",
        "f = lambda x: x**2 - x**3 + 0.25\n",
        "\n",
        "x_positive = 4*np.random.rand(50)-2\n",
        "y_positive = f(x_positive) + 10*np.random.rand(50)\n",
        "\n",
        "x_negative = 4*np.random.rand(50)-2\n",
        "y_negative = f(x_negative) - 10*np.random.rand(50)\n",
        "\n",
        "x = np.asarray(\n",
        "    [np.concatenate(\n",
        "        (x_positive,\n",
        "         x_negative)),\n",
        "     np.concatenate(\n",
        "         (y_positive,\n",
        "          y_negative))\n",
        "     ]).transpose(1,0)\n",
        "\n",
        "t = np.asarray([1,]*x_positive.shape[0] + [0,]*x_negative.shape[0])\n",
        "\n",
        "plt.title(\"Samples\")\n",
        "plt.scatter(x_positive, y_positive, color='red')\n",
        "plt.scatter(x_negative, y_negative, color='blue')\n",
        "plt.plot(np.linspace(-2,2,200), f(np.linspace(-2,2,200)), color='green')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w08v7GkofWE"
      },
      "source": [
        "\"\"\"Data preparation.\"\"\"\n",
        "\n",
        "mus = np.mean(x, axis = 0)\n",
        "sigma = np.std(x, axis = 0)\n",
        "xn = (x-mus)/sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5BtkURzojwP"
      },
      "source": [
        "# Torch Implementation\n",
        "\n",
        "Relevant documentation:\n",
        "\n",
        "- [Layers and Activation Functions](https://pytorch.org/docs/stable/nn.functional.html)\n",
        "- [Class Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)\n",
        "- [Torch Optim Module](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRylK8OeojGg"
      },
      "source": [
        "\"\"\"\n",
        " Basic Neural Network Class\n",
        " A basic neural network with the architecture 2 - 16 - 2- 1\n",
        " extending the class Module of torch.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP1(nn.Module):\n",
        "    \"\"\" A basic Neural network with 2 inputs 1 output.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MLP1, self).__init__() # Call the constructor of parent class.\n",
        "        # The Perceptron also called Fully Connected are Linear layers in Torch\n",
        "        # We initialize the layers of our network.\n",
        "        self.layer1 = nn.Linear(2, 16)\n",
        "        self.layer2 = nn.Linear(16, 2)\n",
        "        self.layer3 = nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Compute the output of the nn from x.\n",
        "        input:\n",
        "        x (tensor (15, batch size)): x are the feature vectors\n",
        "        output:\n",
        "        y (tensor (1, batchsize)): the logits or predicted probability\n",
        "        p(t = 1 | X = x)\n",
        "        \"\"\"\n",
        "        y = torch.relu(self.layer1(x))\n",
        "        y = torch.relu(self.layer2(y))\n",
        "        y = torch.sigmoid(self.layer3(y))\n",
        "        return y\n",
        "\n",
        "# Construct a new instance of nn.\n",
        "mdl = MLP1()\n",
        "# Check the architecture of the nn.\n",
        "print(mdl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gThFOt9zqdEI"
      },
      "source": [
        "\"\"\" Training\n",
        "- Define a Loss Function -- Criterion.\n",
        "- Define the number or epochs(iterations).\n",
        "- We use an optimizer to take care of the weights update.\n",
        "- Define a lr.\n",
        "- We will record the loss function.\n",
        "- We will measure the accuracy at each epoch.\n",
        "\"\"\"\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(5)\n",
        "\n",
        "mdl = MLP1()\n",
        "# We start with MSE -- L_2\n",
        "criterion = nn.MSELoss()\n",
        "epochs = 30\n",
        "lr = 0.4\n",
        "optimizer = optim.SGD(mdl.parameters(), lr=lr)\n",
        "\n",
        "# We load our examples in tensors\n",
        "x = torch.tensor(xn, dtype=torch.float)\n",
        "target = torch.reshape(torch.tensor(t, dtype=torch.float), (-1, 1))\n",
        "\n",
        "thre_val = 0.5 # The threshold value\n",
        "list_loss = []\n",
        "list_acc = []\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    y = mdl(x)\n",
        "    loss = criterion(y, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # Now we compute the accuracy\n",
        "    pred = torch.ge(y, thre_val)\n",
        "    acc = torch.sum(pred == target)/target.shape[0]\n",
        "    list_loss.append(loss.item())\n",
        "    list_acc.append(acc.item())\n",
        "\n",
        "    \n",
        "figs, axs = plt.subplots(1, 2)\n",
        "axs[0].plot(range(epochs), list_loss)\n",
        "axs[0].set_aspect('auto')\n",
        "axs[0].set(xlim=[0, epochs], ylim=[0, 0.3])\n",
        "axs[0].set(title='Loss',\n",
        "           xlabel='epoch',\n",
        "           ylabel='Loss')\n",
        "\n",
        "axs[1].plot(range(epochs), list_acc)\n",
        "axs[1].set(xlim=[0, epochs], ylim = [0.4, 1])\n",
        "axs[1].set(title='Acc',\n",
        "           xlabel='epoch',\n",
        "           ylabel='Acc')\n",
        "print(f\"Loss: {list_loss[0]} ... {list_loss[-1]}\")\n",
        "print(f\"Acc: {list_acc[0]} ... {list_acc[-1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiwnIwOMql1R"
      },
      "source": [
        "\"\"\"\n",
        "Test\n",
        "For the test we create a new set of samples, we know the formula.\n",
        "\"\"\"\n",
        "\n",
        "x_test_positive = 4*np.random.rand(10) - 2\n",
        "y_test_positive = f(x_test_positive) + 5*np.random.rand(10)\n",
        "x_test_negative = 4*np.random.rand(10) - 2\n",
        "y_test_negative = f(x_test_negative) - 5*np.random.rand(10) \n",
        "\n",
        "x_test = np.asarray(\n",
        "    [np.concatenate(\n",
        "        (x_test_positive,\n",
        "         x_test_negative)),\n",
        "     np.concatenate(\n",
        "         (y_test_positive,\n",
        "          y_test_negative))\n",
        "     ]).transpose(1,0)\n",
        "\n",
        "t_test = np.asarray(\n",
        "    [1,]*x_test_positive.shape[0] + \n",
        "    [0,]*x_test_negative.shape[0])\n",
        "\n",
        "xn_test = (x_test - mus)/sigma\n",
        "\n",
        "y_test = mdl(\n",
        "    torch.tensor(\n",
        "        xn_test,\n",
        "        dtype=torch.float)\n",
        "    ).detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLmDuXmsrGe6"
      },
      "source": [
        "# Tensorflow + Keras\n",
        "\n",
        "Now we are going to use the intuitive high-level API of keras.\n",
        "\n",
        "Relevant documentation:\n",
        "\n",
        "- [Sequential Model](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)\n",
        "- [Layes on Keras](https://www.tensorflow.org/api_docs/python/tf/keras/layers)\n",
        "- [Model Fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model?hl=zh-tw#fit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzwXVeXxq0b5"
      },
      "source": [
        "\"\"\"\n",
        " Basic Neural Network Class\n",
        " A basic neural network with the architecture 2 - 16 - 2- 1\n",
        " using the sequential module.\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "\n",
        "# We pack our model in a Sequential Classa\n",
        "\n",
        "mdl = Sequential()\n",
        "mdl.add(Dense(16, input_dim = 2, activation='relu'))\n",
        "mdl.add(Dense(2, activation='relu'))\n",
        "mdl.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "mdl.compile(loss='mse',\n",
        "            optimizer= tf.keras.optimizers.SGD(\n",
        "                learning_rate=0.4,\n",
        "                name=\"SGD\"),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "mdl.fit(xn, t, epochs=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQEPrDgjnFYC"
      },
      "source": [
        "\"\"\"\n",
        "Test\n",
        "For the previously created data.\n",
        "\"\"\"\n",
        "\n",
        "y_test = mdl.predict(x_test)\n",
        "pred = np.heaviside(y_test-thre_val, 0).reshape(-1)\n",
        "x_pred_pos = x_test[pred==1]\n",
        "x_pred_neg = x_test[pred==0]\n",
        "\n",
        "x_fp = x_test[ (pred==1) & (t_test==0) ]\n",
        "x_fn = x_test[ (pred==0) & (t_test==1) ]\n",
        "\n",
        "acc = np.sum(t_test == pred)/t_test.shape[0]\n",
        "print(f\"Accuracy: {acc}\")\n",
        "\n",
        "plt.scatter(x_pred_pos[:,0], x_pred_pos[:, 1], color = 'red')\n",
        "plt.scatter(x_pred_neg[:,0], x_pred_neg[:, 1], color='blue')\n",
        "\n",
        "plt.scatter(x_fp[:,0], x_fp[:,1], color='red', marker='x',s=200)\n",
        "plt.scatter(x_fn[:,0], x_fn[:,1], color='blue', marker='x', s=200)\n",
        "\n",
        "plt.plot(np.linspace(-2,2,200), f(np.linspace(-2,2,200)), color='green')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}