{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "001 Perceptron Esp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPefGNzXDQXatt2nThBWQFK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/perezm-496/AI4MedImg/blob/main/001_Perceptron_Esp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "N_AD9WZdd5Il",
        "outputId": "2c561972-7949-43dc-d59c-b32ccdfc1081"
      },
      "source": [
        "\"\"\" Simple Plot of the Examples we are going to use.\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# We try to emulate an and gate.\n",
        "\n",
        "# samples\n",
        "\n",
        "# inputs -- we add a bias entry at x^0 = 1\n",
        "x = np.asarray( [ [0, 0],\n",
        "                  [0, 1],\n",
        "                  [1, 0],\n",
        "                  [1, 1]], dtype=np.float)\n",
        "\n",
        "# outputs -- A single output t = x1 and x2\n",
        "t = np.asarray( [0, 0, 0, 1], dtype=np.float )\n",
        "\n",
        "# Positive examples\n",
        "xp = x[t == 1,:]\n",
        "\n",
        "# Negative examples\n",
        "xn = x[t == 0, :]\n",
        "\n",
        "# Scatter Plot of Possitive and Negatives\n",
        "plt.scatter(xp[:,0], xp[:,1], color = 'blue', marker='+')\n",
        "plt.scatter(xn[:,0], xn[:,1], color = 'red', marker='o')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fc5cc3aa110>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCEYKlAXdYx_",
        "outputId": "4d4358bc-f3b2-4b65-8d60-09031b33ec85"
      },
      "source": [
        "\"\"\" Simple Perceptron implemented with Numpy.\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# We try to emulate an and gate.\n",
        "\n",
        "# samples\n",
        "\n",
        "# inputs -- we add a bias entry at x^0 = 1\n",
        "x = np.asarray( [ [1, 0, 0],\n",
        "                  [1, 0, 1],\n",
        "                  [1, 1, 0],\n",
        "                  [1, 1, 1]], dtype=np.float)\n",
        "\n",
        "# outputs -- A single output t = x1 and x2\n",
        "t = np.asarray( [ [0, 0, 0, 1]], dtype=np.float )\n",
        "\n",
        "\n",
        "def h(t):\n",
        "    \"\"\"The usual activation function, Heaviside function. \"\"\"\n",
        "    # return np.asarray(t > 0.0, dtype=np.float)\n",
        "    return np.heaviside(t, 0)\n",
        "\n",
        "# Initialize the weights -- We use random weights\n",
        "W = np.random.rand(1, 3)\n",
        "\n",
        "# We need to compute the output of the neural network.\n",
        "\n",
        "def perceptron_output(x, W, h):\n",
        "    \"\"\"Computes the output of the perceptron.\"\"\"\n",
        "    return h(np.matmul(W, x.transpose()))\n",
        "\n",
        "def error(x, W, h, t):\n",
        "    \"\"\"Computes mse the error on the perceptron.\"\"\"\n",
        "    return np.sum((t - perceptron_output(x, W, h))**2)\n",
        "\n",
        "# We execute the algorithm for updating the weights\n",
        "# We apply the update 5 times.\n",
        "\n",
        "alpha = 0.1 # learning rate\n",
        "for it in range(5):\n",
        "    y = perceptron_output(x, W, h)\n",
        "    W = W - alpha*np.matmul(y-t, x)\n",
        "    print(f\"Iteration: {it} \\t Error: {error(x,W,h,t)}\")\n",
        "    print(W)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0 \t Error: 3.0\n",
            "[[0.56159276 0.53642222 0.8570451 ]]\n",
            "Iteration: 1 \t Error: 3.0\n",
            "[[0.26159276 0.43642222 0.7570451 ]]\n",
            "Iteration: 2 \t Error: 2.0\n",
            "[[-0.03840724  0.33642222  0.6570451 ]]\n",
            "Iteration: 3 \t Error: 1.0\n",
            "[[-0.23840724  0.23642222  0.5570451 ]]\n",
            "Iteration: 4 \t Error: 1.0\n",
            "[[-0.33840724  0.23642222  0.4570451 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwYKNJcodeWn",
        "outputId": "6c492014-5ff0-4cd5-a89a-91afac19d855"
      },
      "source": [
        "\"\"\"Simple perceptron implemented with Torch.\"\"\"\n",
        "\n",
        "# Tensors and arrays are very similar in the programming terminology.\n",
        "\n",
        "import torch\n",
        "\n",
        "# We try to emulate an and gate.\n",
        "# samples\n",
        "\n",
        "x = torch.tensor( [ [1, 0, 0],\n",
        "                  [1, 0, 1],\n",
        "                  [1, 1, 0],\n",
        "                    [1, 1, 1]], dtype=torch.float32)\n",
        "\n",
        "# outputs -- A single output t = x1 and x2\n",
        "t = torch.tensor( [ [0, 0, 0, 1]], dtype=torch.float32 )\n",
        "\n",
        "\n",
        "def h(t):\n",
        "    \"\"\"The usual activation function, Heaviside function. \"\"\"\n",
        "    return torch.heaviside(t, torch.tensor([0], dtype=torch.float32)) # Read torch heaviside\n",
        "\n",
        "# Initialize the weights -- We use random weights\n",
        "W = torch.rand(1,3, dtype=torch.float32)\n",
        "\n",
        "# We need to compute the output of the neural network.\n",
        "def perceptron_output(x, W, h):\n",
        "    \"\"\"Computes the output of the perceptron.\"\"\"\n",
        "    return h(torch.matmul(W, x.transpose(1,0)))\n",
        "\n",
        "def error(y, t):\n",
        "    \"\"\"Computes mse the error on the perceptron.\"\"\"\n",
        "    return torch.sum((y - t)**2)\n",
        "\n",
        "# We execute the algorithm for updating the weights\n",
        "# We apply the update 5 times.\n",
        "\n",
        "alpha = 0.1 # learning rate\n",
        "for it in range(5):\n",
        "    y = perceptron_output(x, W, h)\n",
        "    W = W - alpha*torch.matmul(y-t, x)\n",
        "    print(f\"Iteration: {it} \\t Error: {error(y,t)}\")\n",
        "    print(W)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0 \t Error: 3.0\n",
            "tensor([[-0.2262,  0.5763,  0.0099]])\n",
            "Iteration: 1 \t Error: 1.0\n",
            "tensor([[-0.3262,  0.4763,  0.0099]])\n",
            "Iteration: 2 \t Error: 1.0\n",
            "tensor([[-0.4262,  0.3763,  0.0099]])\n",
            "Iteration: 3 \t Error: 1.0\n",
            "tensor([[-0.3262,  0.4763,  0.1099]])\n",
            "Iteration: 4 \t Error: 1.0\n",
            "tensor([[-0.4262,  0.3763,  0.1099]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vVhvDk4dh57",
        "outputId": "87be4ae2-6837-4429-bb7c-b3e38d2a629d"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# We try to emulate an and gate.\n",
        "\n",
        "# samples\n",
        "# inputs -- we add a bias entry at x^0 = 1.\n",
        "# In tf we need to specify this is an array that won't change.\n",
        "x = tf.constant( [ [1, 0, 0],\n",
        "                  [1, 0, 1],\n",
        "                  [1, 1, 0],\n",
        "                   [1, 1, 1]], dtype=tf.float32)\n",
        "\n",
        "# outputs -- A single output t = x1 and x2\n",
        "t = tf.constant( [ [0, 1, 1, 1]], dtype=tf.float32)\n",
        "\n",
        "\n",
        "def h(t):\n",
        "    \"\"\"The usual activation function, Heaviside function. \"\"\"\n",
        "    return tf.maximum(0, tf.sign(t))\n",
        "\n",
        "# Initialize the weights -- We use random weights\n",
        "W = tf.random.uniform((1,3), dtype=tf.float32)\n",
        "\n",
        "# We need to compute the output of the neural network.\n",
        "\n",
        "def perceptron_output(x, W, h):\n",
        "    \"\"\"Computes the output of the perceptron.\"\"\"\n",
        "    return h(tf.matmul(W,tf.transpose(x)))\n",
        "\n",
        "def error(x, W, h, t):\n",
        "    \"\"\"Computes mse the error on the perceptron.\"\"\"\n",
        "    return tf.math.reduce_sum((t - perceptron_output(x, W, h))**2)\n",
        "\n",
        "# We execute the algorithm for updating the weights\n",
        "# We apply the update 5 times.\n",
        "\n",
        "alpha = 0.1 # learning rate\n",
        "for it in range(5):\n",
        "    y = perceptron_output(x, W, h)\n",
        "    W = W - alpha*tf.matmul(y-t, x)\n",
        "    print(f\"Iteration: {it} \\t Error: {error(x,W,h,t)}\")\n",
        "    print(W)\n",
        "    "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0 \t Error: 1.0\n",
            "tf.Tensor([[0.6643926  0.8057705  0.01167738]], shape=(1, 3), dtype=float32)\n",
            "Iteration: 1 \t Error: 1.0\n",
            "tf.Tensor([[0.56439257 0.8057705  0.01167738]], shape=(1, 3), dtype=float32)\n",
            "Iteration: 2 \t Error: 1.0\n",
            "tf.Tensor([[0.46439257 0.8057705  0.01167738]], shape=(1, 3), dtype=float32)\n",
            "Iteration: 3 \t Error: 1.0\n",
            "tf.Tensor([[0.36439258 0.8057705  0.01167738]], shape=(1, 3), dtype=float32)\n",
            "Iteration: 4 \t Error: 1.0\n",
            "tf.Tensor([[0.26439258 0.8057705  0.01167738]], shape=(1, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZckcyGEQdsqZ"
      },
      "source": [
        "# Ejercicios\n",
        "\n",
        "Los siguientes ejercicios buscan brindarle un poco de práctica con el manejo de el ciclo básico de entrenamiento de la mayoría de redes neurales, evitando las complejidades dadas por el manejo de datos masivos y arquitecturas complejas.\n",
        "\n",
        "## Ejercicio 1\n",
        "\n",
        "Modifique el procedimiento de entrenamiento del perceptron mostrado anteriormente (numpy, tensorflow, keras) para que continue el entrenamiento mientras se cumplan las dos siguientes condiciones:\n",
        "1. El error sea mayor que 0.\n",
        "2. El número de iteraciones sea menor que 1,000.\n",
        "\n",
        "## Ejercicio 2\n",
        "\n",
        "Modifique el procedimiento de entrenamiento del perceptron construido en el ejercicio 1 y ejecutelo con $\\alpha = 0.01$.\n",
        "\n",
        "¿Qué puede notar de diferente entre este caso y el del ejercicio 1?\n",
        "\n",
        "\n",
        "## Ejercicio 3\n",
        "\n",
        "Construya un procedimiento que entrene un perceptron que simule una compuerta lógica or, utilice el esque anterior i.e. con un máximo de 1,000 iteraciones y mientras el errror sea mayor que 0.\n",
        "\n",
        "## Referencias\n",
        "\n",
        "1. [Documentación oficial de Torch](https://pytorch.org/docs/stable/index.html)\n",
        "2. [Documentación oficial de TensorFlow](https://www.tensorflow.org/api_docs)\n",
        "3. [Historia del Perceptron](https://homeweb.csulb.edu/~cwallis/artificialn/History.htm)\n",
        "4. Pattern Recognition and Machine Learning (2006), Christopher M. Bishop, Springer 2006.\n",
        "5. Rosenblatt, Frank (1962).  Principles of neurodynamics.  New York: Spartan.   Cf. Rumelhart, D.E., J. L. McClelland and the PDP Research Group (1986).  Parallel Distributed Processing vol. 1&2.  Cambridge: MIT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OfRAcJ8mQ1P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}